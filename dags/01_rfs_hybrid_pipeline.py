from airflow import DAG
from airflow.providers.ssh.operators.ssh import SSHOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import PythonOperator
from airflow.models.param import Param
from datetime import datetime, timedelta
import sys
import os

# --- KRÄ°TÄ°K: Airflow'un 'src' klasÃ¶rÃ¼nÃ¼ gÃ¶rmesi iÃ§in ---
sys.path.append("/opt/airflow")

# --- Ä°MPORTLAR ---
# 1. ETL ModÃ¼lÃ¼
from src.rfs.features.pipeline import LaptopETLPipeline

# 2. Training ModÃ¼lÃ¼
from src.rfs.models.train import IndustrialTrainer

# --- AYARLAR (Host Makine) ---
VENV_PYTHON_PATH = "/Users/erwin/Developer/ml-dl/rfs-mlops/.venv/bin/python"
PROJECT_PATH = "/Users/erwin/Developer/ml-dl/rfs-mlops"
SCRIPT_NAME = "run_scraping.py"

default_args = {
    "owner": "rfs_team",
    "retries": 0,
    "retry_delay": timedelta(minutes=2),
}

# --- FONKSÄ°YONLAR ---


def run_project_etl():
    """
    Ham veriyi temizler ve 'features' tablosuna yazar.
    """
    print("ğŸš€ Airflow ETL BaÅŸlatÄ±lÄ±yor...")
    try:
        pipeline = LaptopETLPipeline()
        pipeline.run()
        print("âœ… ETL BaÅŸarÄ±yla TamamlandÄ±!")
    except Exception as e:
        print(f"âŒ ETL SÄ±rasÄ±nda Hata: {e}")
        raise e


def run_model_training():
    """
    (YENÄ°) Temiz veriyi alÄ±r, modelleri yarÄ±ÅŸtÄ±rÄ±r ve ÅŸampiyonu MLflow'a kaydeder.
    """
    print("ğŸ§  Model EÄŸitimi BaÅŸlÄ±yor...")

    # Docker iÃ§inde MLflow tracking URI'yÄ± set edelim
    # Bu ayar train.py iÃ§indeki 'localhost' ayarÄ±nÄ± ezer.
    os.environ["MLFLOW_TRACKING_URI"] = "http://rfs_mlflow:5000"

    try:
        trainer = IndustrialTrainer()

        # 1. Benchmark (TÃ¼m modelleri yarÄ±ÅŸtÄ±r)
        winner_model = trainer.run_benchmark()
        print(f"ğŸ† Kazanan Model: {winner_model}")

        # 2. Optimizasyon (KazananÄ± eÄŸit)
        if winner_model:
            trainer.optimize_champion(winner_model)
            print("âœ… Åampiyon model optimize edildi ve MLflow'a kaydedildi.")
        else:
            print("âš ï¸ Benchmark sonucunda uygun model bulunamadÄ±.")

    except Exception as e:
        print(f"âŒ EÄŸitim HatasÄ±: {e}")
        raise e


# --- DAG TANIMI ---

with DAG(
    dag_id="01_rfs_hybrid_pipeline",
    default_args=default_args,
    description="Host(Scraping) -> Docker(ETL) -> Docker(Training)",
    schedule_interval=None,
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["hybrid", "ssh", "scraper", "etl", "training"],
    params={
        "hb_url": Param(
            "https://www.hepsiburada.com/laptop-notebook-dizustu-bilgisayarlar-c-98?puan=3-max&sayfa=",
            type="string",
        ),
        "hb_pages": Param(
            1, type="integer", description="HB Sayfa SayÄ±sÄ±- Her sayfa 36 Ã¼rÃ¼n iÃ§erir"
        ),
        "ty_url": Param(
            "https://www.trendyol.com/sr?wc=103108%2C106084&sst=MOST_RATED",
            type="string",
        ),
        "ty_pages": Param(
            2, type="integer", description="TY Sayfa SayÄ±sÄ± - Her sayfa 16 Ã¼rÃ¼n iÃ§erir"
        ),
    },
) as dag:
    start_pipeline = DummyOperator(task_id="start")

    # --- 1. SCRAPING (SSH - Host Makine) ---
    scrape_hb = SSHOperator(
        task_id="scrape_hepsiburada",
        ssh_conn_id="my_local_mac",
        command=f"""
            export DISPLAY=:0 && 
            cd {PROJECT_PATH} && 
            {VENV_PYTHON_PATH} {SCRIPT_NAME} \
            --site hb \
            --hb-pages {{{{ params.hb_pages }}}} \
            --hb-url "{{{{ params.hb_url }}}}"
        """,
        cmd_timeout=3600,
    )

    scrape_ty = SSHOperator(
        task_id="scrape_trendyol",
        ssh_conn_id="my_local_mac",
        command=f"""
            export DISPLAY=:0 && 
            cd {PROJECT_PATH} && 
            {VENV_PYTHON_PATH} {SCRIPT_NAME} \
            --site ty \
            --ty-pages {{{{ params.ty_pages }}}} \
            --ty-url "{{{{ params.ty_url }}}}"
        """,
        cmd_timeout=3600,
    )

    # --- 2. ETL (PythonOperator - Docker Ä°Ã§i) ---
    etl_process = PythonOperator(
        task_id="etl_feature_engineering", python_callable=run_project_etl
    )

    # --- 3. TRAINING (PythonOperator - Docker Ä°Ã§i) ---
    train_model = PythonOperator(
        task_id="train_model_process", python_callable=run_model_training
    )

    end_pipeline = DummyOperator(task_id="end")

    # --- AKIÅ ÅEMASI ---
    # BaÅŸla -> (HB ve TY Paralel) -> Ä°kisi bitince ETL -> Sonra Training -> BitiÅŸ
    (
        start_pipeline
        >> [scrape_hb, scrape_ty]
        >> etl_process
        >> train_model
        >> end_pipeline
    )
